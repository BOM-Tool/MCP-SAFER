#!/usr/bin/env python3
"""
ì‹¤ì œ DistilBERT ëª¨ë¸ì„ ì‚¬ìš©í•œ DLP Inference ì„œë²„
Medium íƒì§€ëœ í…ìŠ¤íŠ¸ë¥¼ MLë¡œ ì¬ê²€ì¦
"""

import json
import logging
import time
import re
import sys
from typing import List, Dict, Any
from concurrent import futures
import os

import grpc
import torch
from transformers import DistilBertForTokenClassification, DistilBertTokenizerFast

# Python ì¶œë ¥ ë²„í¼ë§ ë¹„í™œì„±í™” (ì¦‰ì‹œ ì¶œë ¥)
# Python 3.7+ í˜¸í™˜ì„± ì²´í¬
if hasattr(sys.stdout, 'reconfigure'):
    try:
        sys.stdout.reconfigure(line_buffering=True)
        sys.stderr.reconfigure(line_buffering=True)
    except Exception:
        pass  # ì¬êµ¬ì„±ì´ ì•ˆë˜ë©´ ê·¸ëƒ¥ ë„˜ì–´ê°

# Proto íŒŒì¼ import
import dlp_inference_pb2
import dlp_inference_pb2_grpc

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DistilBERTDLPService(dlp_inference_pb2_grpc.DLPInferenceServicer):
    """ì‹¤ì œ DistilBERT ëª¨ë¸ì„ ì‚¬ìš©í•œ DLP ì„œë¹„ìŠ¤"""
    
    def __init__(self, model_path: str):
        print(f"ğŸ”§ [PYTHON SERVER] DistilBERTDLPService.__init__ called with model_path: {model_path}", flush=True)
        sys.stdout.flush()
        self.model_path = model_path
        self.model_version = "DistilBERT_v1"
        
        # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
        logger.info(f"DistilBERT ëª¨ë¸ ë¡œë”© ì¤‘: {model_path}")
        print(f"ğŸ”§ [PYTHON SERVER] Loading DistilBERT model from: {model_path}", flush=True)
        sys.stdout.flush()
        
        try:
            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            self.tokenizer = DistilBertTokenizerFast.from_pretrained(model_path)
            
            # ëª¨ë¸ ë¡œë“œ (Token Classification)
            self.model = DistilBertForTokenClassification.from_pretrained(
                model_path,
                local_files_only=True,
                trust_remote_code=True
            )
            
            # GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ GPUë¡œ ì´ë™
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            self.model.to(self.device)
            self.model.eval()
            
            # ë¼ë²¨ ë§¤í•‘ ë¡œë“œ
            with open(os.path.join(model_path, "label_mapping.json"), "r", encoding="utf-8") as f:
                self.label_mapping = json.load(f)
            
            logger.info(f"ëª¨ë¸ ë¡œë”© ì™„ë£Œ. Device: {self.device}")
            logger.info(f"í† í° ë ˆë²¨ ë¼ë²¨: {len(self.label_mapping['id2label'])}ê°œ")
            logger.info(f"ë¯¼ê°ì •ë³´ íƒ€ì…: {len(self.label_mapping['type2id'])}ê°œ")
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            # í´ë°±: ê°„ë‹¨í•œ íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ëŒ€ì²´
            self.model = None
            self.tokenizer = None
            self.device = None
            self.label_mapping = {"id2label": {"0": "0", "1": "1"}, "type2id": {}}
            logger.warning("íŒ¨í„´ ë§¤ì¹­ ëª¨ë“œë¡œ í´ë°±")
    
    def DetectSensitiveInfo(self, request, context):
        """ë¯¼ê°ì •ë³´ íƒì§€ - ì‹¤ì œ ML ëª¨ë¸ ì‚¬ìš©"""
        import sys
        print(f"ğŸš€ [PYTHON ML SERVER] ìš”ì²­ ë°›ìŒ: {request.text[:50] if len(request.text) > 50 else request.text}...", flush=True)
        print(f"ğŸ“¥ [PYTHON ML SERVER] ì „ì²´ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(request.text)}", flush=True)
        logger.info(f"ML ì„œë²„ í˜¸ì¶œë¨: {request.text[:50]}...")
        sys.stdout.flush()
        sys.stderr.flush()
        
        start_time = time.time()
        detections = []
        
        try:
            # ML ëª¨ë¸ì´ ë¡œë“œëœ ê²½ìš°
            if self.model is not None and self.tokenizer is not None:
                # í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ê³  ëª¨ë¸ë¡œ ì˜ˆì¸¡
                inputs = self.tokenizer(
                    request.text,
                    return_tensors="pt",
                    truncation=True,
                    padding=True,
                    max_length=256  # ëª¨ë¸ì´ 256ìœ¼ë¡œ í›ˆë ¨ë¨
                )
                
                # GPUë¡œ ì´ë™
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
                
                # ëª¨ë¸ ì¶”ë¡  (Token Classification)
                with torch.no_grad():
                    outputs = self.model(**inputs)
                    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
                    
                    # í† í°ë³„ ì˜ˆì¸¡ ê²°ê³¼ ì²˜ë¦¬
                    token_predictions = torch.argmax(predictions, dim=-1)
                    token_confidences = torch.max(predictions, dim=-1)[0]
                    
                    # ì…ë ¥ í† í°ë“¤ ë””ì½”ë”©
                    input_ids = inputs['input_ids'][0]
                    tokens = self.tokenizer.convert_ids_to_tokens(input_ids)
                    
                    # ë¯¼ê°í•œ í† í°ë“¤ ì°¾ê¸° (ë¼ë²¨ 1 = ë¯¼ê°)
                    sensitive_tokens = []
                    for i, (token_pred, token_conf) in enumerate(zip(token_predictions[0], token_confidences[0])):
                        if token_pred.item() == 1 and token_conf.item() >= 0.5:
                            sensitive_tokens.append({
                                'token': tokens[i],
                                'confidence': token_conf.item(),
                                'position': i
                            })
                    
                    # ë¯¼ê°í•œ í† í°ì´ ìˆìœ¼ë©´ ë¯¼ê°ì •ë³´ë¡œ ë¶„ë¥˜
                    if sensitive_tokens:
                        # í…ìŠ¤íŠ¸ì—ì„œ ë¯¼ê°ì •ë³´ íŒ¨í„´ ì°¾ê¸°
                        sensitive_patterns = self._get_sensitive_patterns()
                        
                        for pattern_name, pattern in sensitive_patterns.items():
                            matches = re.finditer(pattern, request.text)
                            for match in matches:
                                # ë¯¼ê°í•œ í† í°ë“¤ì˜ í‰ê·  ì‹ ë¢°ë„ ê³„ì‚°
                                avg_confidence = sum(t['confidence'] for t in sensitive_tokens) / len(sensitive_tokens)
                                
                                detection = dlp_inference_pb2.SensitiveDetection(
                                    category=self._get_category(pattern_name),
                                    type=pattern_name,
                                    value=match.group(),
                                    start_position=match.start(),
                                    end_position=match.end(),
                                    confidence=avg_confidence,
                                    reasoning=f"ML Token Classification: ë¯¼ê°í•œ í† í° {len(sensitive_tokens)}ê°œ íƒì§€ (í‰ê·  ì‹ ë¢°ë„: {avg_confidence:.3f})",
                                    source="ML_MODEL"
                                )
                                detections.append(detection)
            else:
                # í´ë°±: íŒ¨í„´ ë§¤ì¹­ ì‚¬ìš©
                logger.info("íŒ¨í„´ ë§¤ì¹­ ëª¨ë“œë¡œ íƒì§€")
                sensitive_patterns = self._get_sensitive_patterns()
                
                for pattern_name, pattern in sensitive_patterns.items():
                    matches = re.finditer(pattern, request.text)
                    for match in matches:
                        detection = dlp_inference_pb2.SensitiveDetection(
                            category=self._get_category(pattern_name),
                            type=pattern_name,
                            value=match.group(),
                            start_position=match.start(),
                            end_position=match.end(),
                            confidence=0.8,  # íŒ¨í„´ ë§¤ì¹­ ê¸°ë³¸ ì‹ ë¢°ë„
                            reasoning=f"íŒ¨í„´ ë§¤ì¹­: {pattern}",
                            source="PATTERN_MATCH"
                        )
                        detections.append(detection)
            
            processing_time = int((time.time() - start_time) * 1000)
            confidence_score = 0.85 if detections else 0.0
            
            response = dlp_inference_pb2.SensitiveInfoResponse(
                detections=detections,
                confidence_score=confidence_score,
                processing_time_ms=processing_time,
                model_version=self.model_version,
                request_id=f"req_{int(time.time())}",
                from_cache=False
            )
            
            logger.info(f"ML ì„œë²„ ì‘ë‹µ: {len(detections)}ê°œ íƒì§€, {processing_time}ms")
            print(f"âœ… [PYTHON ML SERVER] ì‘ë‹µ ì „ì†¡: {len(detections)}ê°œ íƒì§€, {processing_time}ms", flush=True)
            sys.stdout.flush()
            return response
            
        except Exception as e:
            logger.error(f"ML íƒì§€ ì˜¤ë¥˜: {e}")
            print(f"âŒ [PYTHON ML SERVER] ì—ëŸ¬ ë°œìƒ: {e}", flush=True)
            sys.stderr.flush()
            raise
    
    def BatchDetectSensitiveInfo(self, request, context):
        """ë°°ì¹˜ ë¯¼ê°ì •ë³´ íƒì§€"""
        responses = []
        total_start = time.time()
        
        for text_request in request.requests:
            # ê°œë³„ ìš”ì²­ ì²˜ë¦¬
            single_request = dlp_inference_pb2.SensitiveInfoRequest(
                text=text_request.text,
                user_id=text_request.user_id,
                session_id=text_request.session_id,
                categories=text_request.categories,
                include_reasoning=text_request.include_reasoning,
                confidence_threshold=text_request.confidence_threshold
            )
            
            try:
                response = self.DetectSensitiveInfo(single_request, context)
                responses.append(response)
            except Exception as e:
                logger.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                # ì˜¤ë¥˜ ì‘ë‹µ ìƒì„±
                error_response = dlp_inference_pb2.SensitiveInfoResponse(
                    detections=[],
                    confidence_score=0.0,
                    processing_time_ms=0,
                    model_version=self.model_version,
                    request_id=f"error_{int(time.time())}",
                    from_cache=False
                )
                responses.append(error_response)
        
        total_time = int((time.time() - total_start) * 1000)
        
        return dlp_inference_pb2.BatchSensitiveInfoResponse(
            responses=responses,
            total_processing_time_ms=total_time,
            successful_requests=len([r for r in responses if r.detections]),
            failed_requests=len([r for r in responses if not r.detections])
        )
    
    def HealthCheck(self, request, context):
        """í—¬ìŠ¤ì²´í¬"""
        return dlp_inference_pb2.HealthCheckResponse(
            status=dlp_inference_pb2.HealthCheckResponse.SERVING,
            message=f"ML ì„œë²„ ì •ìƒ ì‘ë™ ì¤‘ (ëª¨ë¸: {self.model_version})",
            timestamp=int(time.time())
        )
    
    def _get_sensitive_patterns(self):
        """ë¯¼ê°ì •ë³´ íŒ¨í„´ ì •ì˜"""
        return {
            "ì „í™”ë²ˆí˜¸": r'\d{3}-\d{4}-\d{4}',
            "ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸": r'\d{6}-\d{7}',
            "ì´ë©”ì¼": r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}',
            "ì¹´ë“œë²ˆí˜¸": r'\d{4}-\d{4}-\d{4}-\d{4}',
            "ê³„ì¢Œë²ˆí˜¸": r'\d{3}-\d{2,4}-\d{6}',
        }
    
    def _get_category(self, pattern_name):
        """íŒ¨í„´ëª…ì„ ì¹´í…Œê³ ë¦¬ë¡œ ë³€í™˜"""
        category_map = {
            "ì „í™”ë²ˆí˜¸": "personal_info",
            "ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸": "personal_info", 
            "ì´ë©”ì¼": "personal_info",
            "ì¹´ë“œë²ˆí˜¸": "financial",
            "ê³„ì¢Œë²ˆí˜¸": "financial",
        }
        return category_map.get(pattern_name, "unknown")

def serve():
    """gRPC ì„œë²„ ì‹œì‘"""
    # ëª¨ë¸ ê²½ë¡œ ì„¤ì •
    model_path = "../../../../models/DistilBERT_v1"
    
    # ëª¨ë¸ íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(model_path):
        logger.error(f"ëª¨ë¸ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {model_path}")
        return
    
    # gRPC ì„œë²„ ì˜µì…˜ ì„¤ì • (ë©”ì‹œì§€ í¬ê¸° ì œí•œ)
    options = [
        ('grpc.max_send_message_length', 4 * 1024 * 1024),  # 4MB (í´ë¼ì´ì–¸íŠ¸ì™€ ì¼ì¹˜)
        ('grpc.max_receive_message_length', 4 * 1024 * 1024),  # 4MB (í´ë¼ì´ì–¸íŠ¸ì™€ ì¼ì¹˜)
    ]
    
    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10), options=options)
    
    # ì„œë¹„ìŠ¤ ë“±ë¡
    print(f"ğŸ”§ [PYTHON SERVER] Creating DistilBERTDLPService instance...", flush=True)
    sys.stdout.flush()
    service = DistilBERTDLPService(model_path)
    print(f"ğŸ”§ [PYTHON SERVER] Registering service to gRPC server...", flush=True)
    sys.stdout.flush()
    dlp_inference_pb2_grpc.add_DLPInferenceServicer_to_server(service, server)
    print(f"âœ… [PYTHON SERVER] Service registered successfully", flush=True)
    sys.stdout.flush()
    
    # í¬íŠ¸ ë°”ì¸ë”© (IPv4 + IPv6 ëª¨ë‘ ì§€ì›)
    listen_addr = '0.0.0.0:50051'
    server.add_insecure_port(listen_addr)
    
    logger.info(f"ML ì„œë²„ ì‹œì‘: {listen_addr}")
    logger.info(f"ëª¨ë¸ ê²½ë¡œ: {model_path}")
    print(f"ğŸš€ [PYTHON SERVER] Starting gRPC server on {listen_addr}", flush=True)
    sys.stdout.flush()
    
    server.start()
    print(f"âœ… [PYTHON SERVER] gRPC server started successfully on {listen_addr}", flush=True)
    print(f"âœ… [PYTHON SERVER] Server is listening for requests...", flush=True)
    sys.stdout.flush()
    
    try:
        server.wait_for_termination()
    except KeyboardInterrupt:
        logger.info("ì„œë²„ ì¢…ë£Œ ì¤‘...")
        print(f"ğŸ›‘ [PYTHON SERVER] Shutting down...", flush=True)
        sys.stdout.flush()
        server.stop(0)

if __name__ == '__main__':
    serve()
